{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework1 for Trustworthy Machine Learning\n",
    "\n",
    "作业要求请参考PPT\n",
    "\n",
    "如有任何问题，欢迎发邮件至助教邮箱或在微信群里提问：210013020@stu.pku.edu.cn\n",
    "\n",
    "## Deadline\n",
    "**中文/English: 3/22 release, 4/6 24:00 due**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Install other package if neccesary.\n",
    "%pip install lime==0.1.1.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import package\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.segmentation import slic\n",
    "from torch.autograd import Variable\n",
    "from typing import Callable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test whether the jupyter works well.\n",
    "np.random.seed(10)\n",
    "img = Image.open('./test.JPEG')\n",
    "img = img.convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img = transform(img)\n",
    "img_hwc = img.permute(1, 2, 0)\n",
    "plt.imshow(img_hwc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "img = img.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None,:,None,None]) / self.std.type_as(x)[None,:,None,None]\n",
    "imagenet_mean = (0.485, 0.456, 0.406)\n",
    "imagenet_std = (0.229, 0.224, 0.225)\n",
    "net = resnet50(num_classes=1000, pretrained=True)\n",
    "model = nn.Sequential(Normalize(mean=imagenet_mean, std=imagenet_std), net)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = ImageFolder(root=\"./ImageNet_subset/\", transform=transform)\n",
    "dataset_loader = DataLoader(dataset, batch_size=10, num_workers=6)\n",
    "for image, _ in dataset_loader:\n",
    "    result = model(image)\n",
    "    real_label = torch.argmax(result,dim = -1).numpy()\n",
    "\n",
    "\n",
    "# Print the label\n",
    "for j, label in enumerate(real_label):\n",
    "    class_name = json.load(open(\"imagenet_class_index.json\"))[str(label.item())]\n",
    "    print(\"The label of image {:d} is :\".format(j), class_name[1])\n",
    "\n",
    "\n",
    "# Show the image\n",
    "img_indices = [i for i in range(10)]\n",
    "all_image,_ = next(iter(dataset_loader))\n",
    "all_image = all_image.mul(255).add_(0.5).clamp_(0, 255).permute(0, 2, 3, 1).to('cpu', torch.uint8).numpy()\n",
    "fig, axs = plt.subplots(1, len(img_indices), figsize=(15, 8))\n",
    "for i, img in enumerate(all_image):\n",
    "  axs[i].imshow(img)\n",
    "  axs[i].set_xticks([])\n",
    "  axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lime\n",
    "[Lime](https://github.com/marcotcr/lime) is a package about explaining what machine learning classifiers are doing. We can first use it to observe the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    # input: numpy array, (batches, height, width, channels)\n",
    "    # output: the output of the model.\n",
    "    input = np.transpose(input, (0, 3, 1, 2))  # (batches, channels, height, width)\n",
    "    input_tensor = torch.from_numpy(input).float()\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    return output.detach().cpu().numpy()\n",
    "    ###################################\n",
    "    # write the code here\n",
    "    # return output.detach().numpy()\n",
    "    ###################################\n",
    "\n",
    "def segmentation(input):\n",
    "    # split the image into 200 pieces with the help of segmentaion from skimage\n",
    "    # doc: https://scikit-image.org/docs/stable/api/skimage.segmentation.html#slic\n",
    "    return slic(input, n_segments=200, compactness=1, sigma=1, start_label=1)\n",
    "\n",
    "img_indices = [i for i in range(10)]\n",
    "fig, axs = plt.subplots(1, len(img_indices), figsize=(15, 8))\n",
    "\n",
    "\n",
    "# fix the random seed to make it reproducible\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "#<-----!>\n",
    "for idx, (image,label) in enumerate(zip(all_image,real_label)):\n",
    "    x = (image/255).astype(np.double)\n",
    "    \n",
    "    explaination = explainer.explain_instance(\n",
    "        image=x,\n",
    "        classifier_fn=predict,\n",
    "        labels=(10,),\n",
    "        hide_color=0,\n",
    "        top_labels=1,\n",
    "        num_features=11,\n",
    "        num_samples=1000,\n",
    "        batch_size=10,\n",
    "        segmentation_fn=segmentation,\n",
    "        distance_metric='cosine',\n",
    "        model_regressor=None,\n",
    "        random_seed=10,\n",
    "        # progress_bar=True\n",
    "    )\n",
    "    # Refer the doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explain_instance#lime.lime_image.LimeImageExplainer.explain_instance\n",
    "    ###################################\n",
    "    # write the code here\n",
    "    ###################################\n",
    "\n",
    "\n",
    "    # Turn the result from explainer to the image\n",
    "    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=get_image_and_mask#lime.lime_image.ImageExplanation.get_image_and_mask\n",
    "    lime_img, mask = explaination.get_image_and_mask(label=label.item(),positive_only=False,hide_rest=False,num_features=11,min_weight=0.05)\n",
    "    axs[idx].imshow(lime_img)\n",
    "    axs[idx].set_xticks([])\n",
    "    axs[idx].set_yticks([])\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saliency Map\n",
    "What is Saliency map?\n",
    "\n",
    "The heatmaps that highlight pixels of the input image that contribute the most in the classification task.\n",
    "\n",
    "Ref: https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4\n",
    "\n",
    "We put an image into the model, forward then calculate the loss referring to the label. Therefore, the loss is related to:\n",
    "\n",
    "*   image\n",
    "*   model parameters\n",
    "*   label\n",
    "\n",
    "Generally speaking, we change model parameters to fit \"image\" and \"label\". When backward, we calculate the partial differential value of **loss to model parameters**.\n",
    "\n",
    "Now, we have another look. When we change the image's pixel value, the partial differential value of **loss to image** shows the change in the loss. We can say that it means the importance of the pixel. We can visualize it to demonstrate which part of the image contribute the most to the model's judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tips.\n",
    "# What is permute?\n",
    "# In pytorch, the meaning of each dimension of image tensor is (channels, height, width)\n",
    "# In matplotlib, the meaning of each dimension of image tensor is (height, width, channels)\n",
    "# permute is a tool for permuting dimensions of tensors\n",
    "# For example, img.permute(1, 2, 0) means that,\n",
    "# - 0 dimension is the 1 dimension of the original tensor, which is height\n",
    "# - 1 dimension is the 2 dimension of the original tensor, which is width\n",
    "# - 2 dimension is the 0 dimension of the original tensor, which is channels\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "  return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "def compute_saliency_maps(x, y, model):\n",
    "  # input: the input image, the ground truth label, the model\n",
    "  # output: the saliency maps of the images\n",
    "  # We need to normalize each image, because their gradients might vary in scale, use the \"normalize\" function.\n",
    "\n",
    "  '''\n",
    "    x: (tensor) [10, 3, 224, 224]), float32\n",
    "  '''\n",
    "\n",
    "  model.eval()\n",
    "  x = normalize(x)\n",
    "  x.requires_grad_()\n",
    "  outputs = model(x)\n",
    "  scores = outputs[torch.arange(outputs.size(0)), y]\n",
    "  sum_scores = scores.sum()\n",
    "  sum_scores.backward()\n",
    "  saliency_maps = x.grad.data.abs().max(dim=1)[0]\n",
    "  mins = saliency_maps.amin(dim=(1, 2), keepdim=True)\n",
    "  maxs = saliency_maps.amax(dim=(1, 2), keepdim=True)\n",
    "  saliencies = (saliency_maps - mins) / (maxs - mins + 1e-8)\n",
    "\n",
    "  return saliencies\n",
    "  ###################################\n",
    "  # write the code here\n",
    "  # return saliencies\n",
    "  ###################################\n",
    "\n",
    "image,_ = next(iter(dataset_loader))\n",
    "result = model(image)\n",
    "label = torch.argmax(result,dim = -1)\n",
    "\n",
    "\n",
    "\n",
    "saliencies = compute_saliency_maps(image, label, model)\n",
    "# visualize\n",
    "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
    "\n",
    "for row, target in enumerate([image, saliencies]):\n",
    "  for column, img in enumerate(target):\n",
    "    if row==0:\n",
    "      axs[row][column].imshow(img.permute(1, 2, 0).detach().mul(255).add_(0.5).clamp_(0, 255).to('cpu', torch.uint8).numpy())\n",
    "      axs[row][column].set_xticks([])\n",
    "      axs[row][column].set_yticks([])\n",
    "    else:\n",
    "      axs[row][column].imshow(img.numpy())\n",
    "      axs[row][column].set_xticks([])\n",
    "      axs[row][column].set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Smooth Grad\n",
    "The method of Smooth grad is to randomly add noise to the image and get different heatmaps. The average of the heatmaps would be more robust to noisy gradient.\n",
    "\n",
    "ref: https://arxiv.org/pdf/1706.03825.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root=\"./ImageNet_subset/\", transform=transform)\n",
    "dataset_loader = DataLoader(dataset, batch_size=10, num_workers=6)\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None,:,None,None]) / self.std.type_as(x)[None,:,None,None]\n",
    "imagenet_mean = (0.485, 0.456, 0.406)\n",
    "imagenet_std = (0.229, 0.224, 0.225)\n",
    "model = nn.Sequential(Normalize(mean=imagenet_mean, std=imagenet_std), resnet50(num_classes=1000, pretrained=True))\n",
    "model.eval()\n",
    "def smooth_grad(x, y, model, epoch, param_sigma_multiplier):\n",
    "  # input: the input image, the ground truth label, the model, the number for average, Sigma multiplier when calculating std of noise\n",
    "  # output: the saliency maps of the images\n",
    "\n",
    "  sigma = param_sigma_multiplier * (x.max() - x.min())\n",
    "  x_grad = torch.zeros(size=[epoch, *x.shape])\n",
    "\n",
    "  print(f'Image {x.shape} working')\n",
    "  one_hot = nn.functional.one_hot(y, num_classes=1000).float() # TODO\n",
    "  one_hot = one_hot.unsqueeze(0)  # 添加一个批量维度，形状变为 [1, 1000]\n",
    "\n",
    "  for i in range(epoch):\n",
    "    print(f'    epoch{i}')\n",
    "    with torch.no_grad():\n",
    "      x_noise = x.detach() + torch.randn(x.shape) * sigma\n",
    "    x_noise.requires_grad = True\n",
    "    output = model(x_noise)\n",
    "    # print(output.shape) 1, 1000\n",
    "    output.backward(one_hot)\n",
    "    x_grad[i] = x_noise.grad\n",
    "    print(x_noise.grad.shape).clone()\n",
    "    x_noise.grad = None  # 显式释放梯度\n",
    "    torch.cuda.empty_cache()  # 清理显存（若使用GPU）\n",
    "  return x_grad.mean(dim=0)\n",
    "     \n",
    "\n",
    "\n",
    "smooth = []\n",
    "image,_ = next(iter(dataset_loader))\n",
    "result = model(image)\n",
    "label = torch.argmax(result,dim = -1)\n",
    "for i, l in zip(image, label):\n",
    "  smooth.append(smooth_grad(i, l, model, 10, 0.4))\n",
    "smooth = np.stack(smooth)\n",
    "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
    "for row, target in enumerate([image, smooth]):\n",
    "  for column, img in enumerate(target):\n",
    "    if row==0:\n",
    "        axs[row][column].imshow(img.permute(1, 2, 0).detach().mul(255).add_(0.5).clamp_(0, 255).to('cpu', torch.uint8).numpy())\n",
    "        axs[row][column].set_xticks([])\n",
    "        axs[row][column].set_yticks([])\n",
    "    else:\n",
    "        axs[row][column].imshow(np.transpose(img.reshape(3,224,224), (1,2,0)))\n",
    "        axs[row][column].set_xticks([])\n",
    "        axs[row][column].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Grad-cam\n",
    "The method of Grad-cam is used to get the heatmaps of the models through the calculated gradients on the intermediate features.\n",
    "\n",
    "ref: https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scale_cam_image(cam, target_size=None):\n",
    "    result = []\n",
    "    for img in cam:\n",
    "        img = img - np.min(img)\n",
    "        img = img / (1e-7 + np.max(img))\n",
    "        if target_size is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "        result.append(img)\n",
    "    result = np.float32(result)\n",
    "    return result\n",
    "\n",
    "class ActivationsAndGradients:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        self.handles = []\n",
    "        for target_layer in target_layers:\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_activation))\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_gradient))\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        activation = output\n",
    "        self.activations.append(activation.cpu().detach())\n",
    "\n",
    "    def save_gradient(self, module, input, output):\n",
    "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
    "            # You can only register hooks on tensor requires grad.\n",
    "            return\n",
    "        # Gradients are computed in reverse order\n",
    "        def _store_grad(grad):\n",
    "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
    "        output.register_hook(_store_grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        return self.model(x)\n",
    "\n",
    "    def release(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_cam_on_image(img: np.ndarray,\n",
    "                      mask: np.ndarray,\n",
    "                      use_rgb: bool = False,\n",
    "                      colormap: int = cv2.COLORMAP_JET,\n",
    "                      image_weight: float = 0.5) -> np.ndarray:\n",
    "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
    "    By default the heatmap is in BGR format.\n",
    "    :param img: The base image in RGB or BGR format.\n",
    "    :param mask: The cam mask.\n",
    "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
    "    :param colormap: The OpenCV colormap to be used.\n",
    "    :param image_weight: The final result is image_weight * img + (1-image_weight) * mask.\n",
    "    :returns: The default image with the cam overlay.\n",
    "    \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    if use_rgb:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    if np.max(img) > 1:\n",
    "        raise Exception(\n",
    "            \"The input image should np.float32 in the range [0, 1]\")\n",
    "\n",
    "    if image_weight < 0 or image_weight > 1:\n",
    "        raise Exception(\n",
    "            f\"image_weight should be in the range [0, 1].\\\n",
    "                Got: {image_weight}\")\n",
    "\n",
    "    cam = (1 - image_weight) * heatmap + image_weight * img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "\n",
    "class ClassifierOutputTarget:\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        if len(model_output.shape) == 1:\n",
    "            return model_output[self.category]\n",
    "        return model_output[:, self.category]\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 target_layers: List[torch.nn.Module]) -> None:\n",
    "        self.model = model.eval()\n",
    "        self.target_layers = target_layers\n",
    "        self.model = model\n",
    "        self.activations_and_grads = ActivationsAndGradients(\n",
    "            self.model, target_layers)\n",
    "\n",
    "    \"\"\" Get a vector of weights for every channel in the target layer.\n",
    "        Methods that return weights channels,\n",
    "        will typically need to only implement this function. \"\"\"\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        grads: torch.Tensor) -> np.ndarray:\n",
    "        return np.mean(grads, axis=(2, 3))\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      activations: torch.Tensor,\n",
    "                      grads: torch.Tensor) -> np.ndarray:\n",
    "        # input: the activation, the gradient(4D tensor)\n",
    "        # output: cam of a specific layer\n",
    "        pass\n",
    "        ###################################\n",
    "        # write the code here\n",
    "        # return cam\n",
    "        ###################################\n",
    "\n",
    "    def forward(self,\n",
    "                input_tensor: torch.Tensor,\n",
    "                targets: List[torch.nn.Module]) -> np.ndarray:\n",
    "        outputs = self.activations_and_grads(input_tensor)\n",
    "        self.model.zero_grad()\n",
    "        loss = sum([target(output)\n",
    "                   for target, output in zip(targets, outputs)])\n",
    "        loss.backward(retain_graph=True)\n",
    "        # In most of the saliency attribution papers, the saliency is\n",
    "        # computed with a single target layer.\n",
    "        # Commonly it is the last convolutional layer.\n",
    "        # Here we support passing a list with multiple target layers.\n",
    "        # It will compute the saliency image for every image,\n",
    "        # and then aggregate them (with a default mean aggregation).\n",
    "        # This gives you more flexibility in case you just want to\n",
    "        # use all conv layers for example, all Batchnorm layers,\n",
    "        # or something else.\n",
    "        cam_per_layer = self.compute_cam_per_layer(input_tensor)\n",
    "        return self.aggregate_multi_layers(cam_per_layer)\n",
    "\n",
    "\n",
    "    def compute_cam_per_layer(\n",
    "            self,\n",
    "            input_tensor: torch.Tensor) -> np.ndarray:\n",
    "        activations_list = [a.cpu().data.numpy()\n",
    "                            for a in self.activations_and_grads.activations]\n",
    "        grads_list = [g.cpu().data.numpy()\n",
    "                      for g in self.activations_and_grads.gradients]\n",
    "        target_size =  (input_tensor.size(-1), input_tensor.size(-2))\n",
    "        cam_per_target_layer = []\n",
    "        # Loop over the saliency image from every layer\n",
    "        for i in range(len(self.target_layers)):\n",
    "            target_layer = self.target_layers[i]\n",
    "            layer_activations = None\n",
    "            layer_grads = None\n",
    "            if i < len(activations_list):\n",
    "                layer_activations = activations_list[i]\n",
    "            if i < len(grads_list):\n",
    "                layer_grads = grads_list[i]\n",
    "            ###################################\n",
    "            # write the code here\n",
    "            ###################################\n",
    "        return cam_per_target_layer\n",
    "\n",
    "    def aggregate_multi_layers(\n",
    "            self,\n",
    "            cam_per_target_layer: np.ndarray) -> np.ndarray:\n",
    "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
    "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
    "        result = np.mean(cam_per_target_layer, axis=1)\n",
    "        return scale_cam_image(result)\n",
    "\n",
    "\n",
    "    def __call__(self,\n",
    "                 input_tensor: torch.Tensor,\n",
    "                 targets: List[torch.nn.Module] = None) -> np.ndarray:\n",
    "        return self.forward(input_tensor,\n",
    "                            targets)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.activations_and_grads.release()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.activations_and_grads.release()\n",
    "        if isinstance(exc_value, IndexError):\n",
    "            # Handle IndexError here...\n",
    "            print(\n",
    "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "label = torch.argmax(result,dim = -1).numpy()\n",
    "target_layers = [model.layer4[-1]]\n",
    "image,_ = next(iter(dataset_loader))\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "targets = [ClassifierOutputTarget(i) for i in label]\n",
    "grayscale_cam = cam(input_tensor=image, targets=targets)\n",
    "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
    "for column, single_image in enumerate(image):\n",
    "    axs[0][column].imshow(single_image.permute(1, 2, 0).detach().mul(255).add_(0.5).clamp_(0, 255).to('cpu', torch.uint8).numpy())\n",
    "    axs[0][column].set_xticks([])\n",
    "    axs[0][column].set_yticks([])\n",
    "    axs[1][column].imshow(show_cam_on_image(single_image.permute(1, 2, 0).detach().to('cpu').numpy(),grayscale_cam[column],use_rgb=True))\n",
    "    axs[1][column].set_xticks([])\n",
    "    axs[1][column].set_yticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
